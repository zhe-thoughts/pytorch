import dataclasses
from torch.fx import Node, Graph
import torch
from typing import Optional
from .utils import use_tangent, compute_tensor_size
from torch._inductor import config

aten = torch.ops.aten

class CantChunk(RuntimeError):
    pass

@dataclasses.dataclass
class ChunkingMeta:
    # The value of the node should be scaled by the specified scalar
    # tensor. Need this sine we pretend tangent to be 1 first and
    # scale the affected tensor later. Need propagate such information
    # to downstream.
    scale_by: Node = None 

    # The dimension of the current tensor that get chunked.
    # Can be None if the current tensor is not chunked. E.g., when
    # the current tensor is a scalar tensor generated by summing a chunked tensor.
    #
    # To recover a tensor with non-None chunk_dim, we need concat each
    # chunk at the 'chunk_dim' dimension.
    chunk_dim: Optional[int] = None

    # The original tensor is the sum of each tensor in the chunk subgraph.
    # chunk_dim should be None if need_sum is True
    #
    # Note for some special cases like the tangent placeholder node, both
    # chunk_dim can be None and need_sum can be false, but scale_by
    # in that case is the tangent node itself.
    need_sum: bool = False

    def copy(self, **kwargs):
        meta = ChunkingMeta(**self.__dict__)
        for k, v in kwargs.items():
            setattr(meta, k, v)
        return meta

    @staticmethod
    def equal(lhs_meta, rhs_meta, skip_scale_by=False):
        if skip_scale_by:
            lhs_meta = lhs_meta.copy(scale_by=None)
            rhs_meta = rhs_meta.copy(scale_by=None)
        return lhs_meta == rhs_meta

    def chunk_by_dim(self, dim: int):
        return self.chunk_dim == dim

    @staticmethod
    def is_nop(meta):
        return meta is None or meta == ChunkingMeta()

def set_chunking_meta(node, meta=None, **kwargs):
    """
    kwargs can override fields in the passed in `meta`
    """
    changed = False
    if meta is None:
        meta = ChunkingMeta(**kwargs)
    else:
        # make a copy to avoid override the passed in instance
        meta = meta.copy()
        for k, v in kwargs.items():
            setattr(meta, k, v)

    old_meta = get_chunking_meta(node)
    node.meta["chunking"] = meta
    return old_meta is None or old_meta != meta

def update_chunking_meta(node, **kwargs):
    """
    Unlike set_chunking_mete, this function keeps the existing chunking
    metadata if it's not overriden.
    """
    meta = get_chunking_meta(node)
    if meta is None:
        meta = ChunkingMeta()
    for k, v in kwargs.items():
        setattr(meta, k, v)

    node.meta["chunking"] = meta

def set_chunking_meta_if_none(nodes, meta):
    changed = False
    for node in nodes:
        if get_chunking_meta(node) is None:
            changed = True
            set_chunking_meta(node, meta)
    return changed
            

def copy_chunking_meta(dst_node, src_node):
    if isinstance(src_node, torch.fx.Node):
        src_meta = get_chunking_meta(src_node)
    else:
        assert isinstance(src_node, ChunkingMeta)
        src_meta = src_node
    assert src_meta
    return set_chunking_meta(dst_node, **src_meta.__dict__)


def get_chunking_meta(node):
    return node.meta.get("chunking")

def has_nop_chunking_meta(node):
    return ChunkingMeta.is_nop(get_chunking_meta(node))

def get_chunking_metas(nodes):
    return [get_chunking_meta(node) for node in nodes]

eligible_amplifier_node = {
    aten.mm.default,
    aten.addmm.default,
}

def find_amplifier_node(graph: Graph) -> Optional[Node]:
    r"""
    Find the 'amplifier' node which is a not that generates large
    output with small/medium input.

    If there are multiple amplifier nodes, return the one with the largest
    amplification ratio.
    """

    amplifier_nodes_ratio = []
    for node in graph.nodes:
        # We only look for amplifier nodes in the fwd part of the graph
        if use_tangent(node):
            break

    # A source user is the user of a source node that we want to
    # chunk. The source node is node we start chunking.
    source_users = []
    for node in graph.nodes:
        if use_tangent(node):
            # enter backward part of the graph
            break

        # Only trigger chunking for a small set of nodes like matmul for now
        if node.op != "call_function" or node.target not in eligible_amplifier_node:
            continue

        input_size = compute_tensor_size(node.args, node.kwargs)
        output_size = compute_tensor_size(node)

        if input_size == 0:
            continue

        ratio = output_size / input_size
        if output_size > config.AutoChunker.output_size_threshold and ratio > config.AutoChunker.amplify_ratio_threshold:
            amplifier_nodes_ratio.append((node, ratio))

    amplifier_nodes_ratio = sorted(amplifier_nodes_ratio, key=lambda x: x[1], reverse=True)
    return amplifier_nodes_ratio[0][0] if len(amplifier_nodes_ratio) > 0 else None
